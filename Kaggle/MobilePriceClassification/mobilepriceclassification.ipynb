{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mobile-price-classification/train.csv\n",
      "/kaggle/input/mobile-price-classification/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mobile-price-classification']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('/kaggle/input'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.0000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1238.518500</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>1.522250</td>\n",
       "      <td>0.509500</td>\n",
       "      <td>4.309500</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>32.046500</td>\n",
       "      <td>0.501750</td>\n",
       "      <td>140.249000</td>\n",
       "      <td>4.520500</td>\n",
       "      <td>...</td>\n",
       "      <td>645.108000</td>\n",
       "      <td>1251.515500</td>\n",
       "      <td>2124.213000</td>\n",
       "      <td>12.306500</td>\n",
       "      <td>5.767000</td>\n",
       "      <td>11.011000</td>\n",
       "      <td>0.761500</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.507000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>439.418206</td>\n",
       "      <td>0.5001</td>\n",
       "      <td>0.816004</td>\n",
       "      <td>0.500035</td>\n",
       "      <td>4.341444</td>\n",
       "      <td>0.499662</td>\n",
       "      <td>18.145715</td>\n",
       "      <td>0.288416</td>\n",
       "      <td>35.399655</td>\n",
       "      <td>2.287837</td>\n",
       "      <td>...</td>\n",
       "      <td>443.780811</td>\n",
       "      <td>432.199447</td>\n",
       "      <td>1084.732044</td>\n",
       "      <td>4.213245</td>\n",
       "      <td>4.356398</td>\n",
       "      <td>5.463955</td>\n",
       "      <td>0.426273</td>\n",
       "      <td>0.500116</td>\n",
       "      <td>0.500076</td>\n",
       "      <td>1.118314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>501.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>851.750000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>282.750000</td>\n",
       "      <td>874.750000</td>\n",
       "      <td>1207.500000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1226.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1247.000000</td>\n",
       "      <td>2146.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1615.250000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>947.250000</td>\n",
       "      <td>1633.000000</td>\n",
       "      <td>3064.500000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1998.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>1998.000000</td>\n",
       "      <td>3998.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       battery_power       blue  clock_speed     dual_sim           fc  \\\n",
       "count    2000.000000  2000.0000  2000.000000  2000.000000  2000.000000   \n",
       "mean     1238.518500     0.4950     1.522250     0.509500     4.309500   \n",
       "std       439.418206     0.5001     0.816004     0.500035     4.341444   \n",
       "min       501.000000     0.0000     0.500000     0.000000     0.000000   \n",
       "25%       851.750000     0.0000     0.700000     0.000000     1.000000   \n",
       "50%      1226.000000     0.0000     1.500000     1.000000     3.000000   \n",
       "75%      1615.250000     1.0000     2.200000     1.000000     7.000000   \n",
       "max      1998.000000     1.0000     3.000000     1.000000    19.000000   \n",
       "\n",
       "            four_g   int_memory        m_dep    mobile_wt      n_cores  ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
       "mean      0.521500    32.046500     0.501750   140.249000     4.520500  ...   \n",
       "std       0.499662    18.145715     0.288416    35.399655     2.287837  ...   \n",
       "min       0.000000     2.000000     0.100000    80.000000     1.000000  ...   \n",
       "25%       0.000000    16.000000     0.200000   109.000000     3.000000  ...   \n",
       "50%       1.000000    32.000000     0.500000   141.000000     4.000000  ...   \n",
       "75%       1.000000    48.000000     0.800000   170.000000     7.000000  ...   \n",
       "max       1.000000    64.000000     1.000000   200.000000     8.000000  ...   \n",
       "\n",
       "         px_height     px_width          ram         sc_h         sc_w  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean    645.108000  1251.515500  2124.213000    12.306500     5.767000   \n",
       "std     443.780811   432.199447  1084.732044     4.213245     4.356398   \n",
       "min       0.000000   500.000000   256.000000     5.000000     0.000000   \n",
       "25%     282.750000   874.750000  1207.500000     9.000000     2.000000   \n",
       "50%     564.000000  1247.000000  2146.500000    12.000000     5.000000   \n",
       "75%     947.250000  1633.000000  3064.500000    16.000000     9.000000   \n",
       "max    1960.000000  1998.000000  3998.000000    19.000000    18.000000   \n",
       "\n",
       "         talk_time      three_g  touch_screen         wifi  price_range  \n",
       "count  2000.000000  2000.000000   2000.000000  2000.000000  2000.000000  \n",
       "mean     11.011000     0.761500      0.503000     0.507000     1.500000  \n",
       "std       5.463955     0.426273      0.500116     0.500076     1.118314  \n",
       "min       2.000000     0.000000      0.000000     0.000000     0.000000  \n",
       "25%       6.000000     1.000000      0.000000     0.000000     0.750000  \n",
       "50%      11.000000     1.000000      1.000000     1.000000     1.500000  \n",
       "75%      16.000000     1.000000      1.000000     1.000000     2.250000  \n",
       "max      20.000000     1.000000      1.000000     1.000000     3.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frame = pd.read_csv('/kaggle/input/mobile-price-classification/train.csv')\n",
    "test_frame = pd.read_csv('/kaggle/input/mobile-price-classification/test.csv')\n",
    "train_frame.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>battery_power</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011252</td>\n",
       "      <td>0.011482</td>\n",
       "      <td>-0.041847</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>-0.004004</td>\n",
       "      <td>0.034085</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>-0.029727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014901</td>\n",
       "      <td>-0.008402</td>\n",
       "      <td>-0.000653</td>\n",
       "      <td>-0.029959</td>\n",
       "      <td>-0.021421</td>\n",
       "      <td>0.052510</td>\n",
       "      <td>0.011522</td>\n",
       "      <td>-0.010516</td>\n",
       "      <td>-0.008343</td>\n",
       "      <td>0.200723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>0.011252</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021419</td>\n",
       "      <td>0.035198</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>0.041177</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>-0.008605</td>\n",
       "      <td>0.036161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006872</td>\n",
       "      <td>-0.041533</td>\n",
       "      <td>0.026351</td>\n",
       "      <td>-0.002952</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.013934</td>\n",
       "      <td>-0.030236</td>\n",
       "      <td>0.010061</td>\n",
       "      <td>-0.021863</td>\n",
       "      <td>0.020573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clock_speed</th>\n",
       "      <td>0.011482</td>\n",
       "      <td>0.021419</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>-0.000434</td>\n",
       "      <td>-0.043073</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>-0.014364</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>-0.005724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014523</td>\n",
       "      <td>-0.009476</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>-0.029078</td>\n",
       "      <td>-0.007378</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.046433</td>\n",
       "      <td>0.019756</td>\n",
       "      <td>-0.024471</td>\n",
       "      <td>-0.006606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dual_sim</th>\n",
       "      <td>-0.041847</td>\n",
       "      <td>0.035198</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.029123</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>-0.015679</td>\n",
       "      <td>-0.022142</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>-0.024658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020875</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>0.041072</td>\n",
       "      <td>-0.011949</td>\n",
       "      <td>-0.016666</td>\n",
       "      <td>-0.039404</td>\n",
       "      <td>-0.014008</td>\n",
       "      <td>-0.017117</td>\n",
       "      <td>0.022740</td>\n",
       "      <td>0.017444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fc</th>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>-0.000434</td>\n",
       "      <td>-0.029123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.016560</td>\n",
       "      <td>-0.029133</td>\n",
       "      <td>-0.001791</td>\n",
       "      <td>0.023618</td>\n",
       "      <td>-0.013356</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009990</td>\n",
       "      <td>-0.005176</td>\n",
       "      <td>0.015099</td>\n",
       "      <td>-0.011014</td>\n",
       "      <td>-0.012373</td>\n",
       "      <td>-0.006829</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>-0.014828</td>\n",
       "      <td>0.020085</td>\n",
       "      <td>0.021998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>four_g</th>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>-0.043073</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>-0.016560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>-0.001823</td>\n",
       "      <td>-0.016537</td>\n",
       "      <td>-0.029706</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019236</td>\n",
       "      <td>0.007448</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.027166</td>\n",
       "      <td>0.037005</td>\n",
       "      <td>-0.046628</td>\n",
       "      <td>0.584246</td>\n",
       "      <td>0.016758</td>\n",
       "      <td>-0.017620</td>\n",
       "      <td>0.014772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int_memory</th>\n",
       "      <td>-0.004004</td>\n",
       "      <td>0.041177</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>-0.015679</td>\n",
       "      <td>-0.029133</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>-0.034214</td>\n",
       "      <td>-0.028310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>-0.008335</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>0.037771</td>\n",
       "      <td>0.011731</td>\n",
       "      <td>-0.002790</td>\n",
       "      <td>-0.009366</td>\n",
       "      <td>-0.026999</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.044435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m_dep</th>\n",
       "      <td>0.034085</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>-0.014364</td>\n",
       "      <td>-0.022142</td>\n",
       "      <td>-0.001791</td>\n",
       "      <td>-0.001823</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021756</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025263</td>\n",
       "      <td>0.023566</td>\n",
       "      <td>-0.009434</td>\n",
       "      <td>-0.025348</td>\n",
       "      <td>-0.018388</td>\n",
       "      <td>0.017003</td>\n",
       "      <td>-0.012065</td>\n",
       "      <td>-0.002638</td>\n",
       "      <td>-0.028353</td>\n",
       "      <td>0.000853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobile_wt</th>\n",
       "      <td>0.001844</td>\n",
       "      <td>-0.008605</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>-0.008979</td>\n",
       "      <td>0.023618</td>\n",
       "      <td>-0.016537</td>\n",
       "      <td>-0.034214</td>\n",
       "      <td>0.021756</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>-0.002581</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.020761</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>-0.014368</td>\n",
       "      <td>-0.000409</td>\n",
       "      <td>-0.030302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_cores</th>\n",
       "      <td>-0.029727</td>\n",
       "      <td>0.036161</td>\n",
       "      <td>-0.005724</td>\n",
       "      <td>-0.024658</td>\n",
       "      <td>-0.013356</td>\n",
       "      <td>-0.029706</td>\n",
       "      <td>-0.028310</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>-0.018989</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006872</td>\n",
       "      <td>0.024480</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>0.025826</td>\n",
       "      <td>0.013148</td>\n",
       "      <td>-0.014733</td>\n",
       "      <td>0.023774</td>\n",
       "      <td>-0.009964</td>\n",
       "      <td>0.004399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pc</th>\n",
       "      <td>0.031441</td>\n",
       "      <td>-0.009952</td>\n",
       "      <td>-0.005245</td>\n",
       "      <td>-0.017143</td>\n",
       "      <td>0.644595</td>\n",
       "      <td>-0.005598</td>\n",
       "      <td>-0.033273</td>\n",
       "      <td>0.026282</td>\n",
       "      <td>0.018844</td>\n",
       "      <td>-0.001193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018465</td>\n",
       "      <td>0.004196</td>\n",
       "      <td>0.028984</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>-0.023819</td>\n",
       "      <td>0.014657</td>\n",
       "      <td>-0.001322</td>\n",
       "      <td>-0.008742</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>0.033599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>px_height</th>\n",
       "      <td>0.014901</td>\n",
       "      <td>-0.006872</td>\n",
       "      <td>-0.014523</td>\n",
       "      <td>-0.020875</td>\n",
       "      <td>-0.009990</td>\n",
       "      <td>-0.019236</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.025263</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>-0.006872</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.510664</td>\n",
       "      <td>-0.020352</td>\n",
       "      <td>0.059615</td>\n",
       "      <td>0.043038</td>\n",
       "      <td>-0.010645</td>\n",
       "      <td>-0.031174</td>\n",
       "      <td>0.021891</td>\n",
       "      <td>0.051824</td>\n",
       "      <td>0.148858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>px_width</th>\n",
       "      <td>-0.008402</td>\n",
       "      <td>-0.041533</td>\n",
       "      <td>-0.009476</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>-0.005176</td>\n",
       "      <td>0.007448</td>\n",
       "      <td>-0.008335</td>\n",
       "      <td>0.023566</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.024480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.021599</td>\n",
       "      <td>0.034699</td>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>-0.001628</td>\n",
       "      <td>0.030319</td>\n",
       "      <td>0.165818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ram</th>\n",
       "      <td>-0.000653</td>\n",
       "      <td>0.026351</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.041072</td>\n",
       "      <td>0.015099</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>-0.009434</td>\n",
       "      <td>-0.002581</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020352</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015996</td>\n",
       "      <td>0.035576</td>\n",
       "      <td>0.010820</td>\n",
       "      <td>0.015795</td>\n",
       "      <td>-0.030455</td>\n",
       "      <td>0.022669</td>\n",
       "      <td>0.917046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sc_h</th>\n",
       "      <td>-0.029959</td>\n",
       "      <td>-0.002952</td>\n",
       "      <td>-0.029078</td>\n",
       "      <td>-0.011949</td>\n",
       "      <td>-0.011014</td>\n",
       "      <td>0.027166</td>\n",
       "      <td>0.037771</td>\n",
       "      <td>-0.025348</td>\n",
       "      <td>-0.033855</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059615</td>\n",
       "      <td>0.021599</td>\n",
       "      <td>0.015996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506144</td>\n",
       "      <td>-0.017335</td>\n",
       "      <td>0.012033</td>\n",
       "      <td>-0.020023</td>\n",
       "      <td>0.025929</td>\n",
       "      <td>0.022986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sc_w</th>\n",
       "      <td>-0.021421</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>-0.007378</td>\n",
       "      <td>-0.016666</td>\n",
       "      <td>-0.012373</td>\n",
       "      <td>0.037005</td>\n",
       "      <td>0.011731</td>\n",
       "      <td>-0.018388</td>\n",
       "      <td>-0.020761</td>\n",
       "      <td>0.025826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043038</td>\n",
       "      <td>0.034699</td>\n",
       "      <td>0.035576</td>\n",
       "      <td>0.506144</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022821</td>\n",
       "      <td>0.030941</td>\n",
       "      <td>0.012720</td>\n",
       "      <td>0.035423</td>\n",
       "      <td>0.038711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk_time</th>\n",
       "      <td>0.052510</td>\n",
       "      <td>0.013934</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.039404</td>\n",
       "      <td>-0.006829</td>\n",
       "      <td>-0.046628</td>\n",
       "      <td>-0.002790</td>\n",
       "      <td>0.017003</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>0.013148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010645</td>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.010820</td>\n",
       "      <td>-0.017335</td>\n",
       "      <td>-0.022821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.042688</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>-0.029504</td>\n",
       "      <td>0.021859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>three_g</th>\n",
       "      <td>0.011522</td>\n",
       "      <td>-0.030236</td>\n",
       "      <td>-0.046433</td>\n",
       "      <td>-0.014008</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.584246</td>\n",
       "      <td>-0.009366</td>\n",
       "      <td>-0.012065</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>-0.014733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031174</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.015795</td>\n",
       "      <td>0.012033</td>\n",
       "      <td>0.030941</td>\n",
       "      <td>-0.042688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.023611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>touch_screen</th>\n",
       "      <td>-0.010516</td>\n",
       "      <td>0.010061</td>\n",
       "      <td>0.019756</td>\n",
       "      <td>-0.017117</td>\n",
       "      <td>-0.014828</td>\n",
       "      <td>0.016758</td>\n",
       "      <td>-0.026999</td>\n",
       "      <td>-0.002638</td>\n",
       "      <td>-0.014368</td>\n",
       "      <td>0.023774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021891</td>\n",
       "      <td>-0.001628</td>\n",
       "      <td>-0.030455</td>\n",
       "      <td>-0.020023</td>\n",
       "      <td>0.012720</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011917</td>\n",
       "      <td>-0.030411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wifi</th>\n",
       "      <td>-0.008343</td>\n",
       "      <td>-0.021863</td>\n",
       "      <td>-0.024471</td>\n",
       "      <td>0.022740</td>\n",
       "      <td>0.020085</td>\n",
       "      <td>-0.017620</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>-0.028353</td>\n",
       "      <td>-0.000409</td>\n",
       "      <td>-0.009964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051824</td>\n",
       "      <td>0.030319</td>\n",
       "      <td>0.022669</td>\n",
       "      <td>0.025929</td>\n",
       "      <td>0.035423</td>\n",
       "      <td>-0.029504</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.011917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_range</th>\n",
       "      <td>0.200723</td>\n",
       "      <td>0.020573</td>\n",
       "      <td>-0.006606</td>\n",
       "      <td>0.017444</td>\n",
       "      <td>0.021998</td>\n",
       "      <td>0.014772</td>\n",
       "      <td>0.044435</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>-0.030302</td>\n",
       "      <td>0.004399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148858</td>\n",
       "      <td>0.165818</td>\n",
       "      <td>0.917046</td>\n",
       "      <td>0.022986</td>\n",
       "      <td>0.038711</td>\n",
       "      <td>0.021859</td>\n",
       "      <td>0.023611</td>\n",
       "      <td>-0.030411</td>\n",
       "      <td>0.018785</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               battery_power      blue  clock_speed  dual_sim        fc  \\\n",
       "battery_power       1.000000  0.011252     0.011482 -0.041847  0.033334   \n",
       "blue                0.011252  1.000000     0.021419  0.035198  0.003593   \n",
       "clock_speed         0.011482  0.021419     1.000000 -0.001315 -0.000434   \n",
       "dual_sim           -0.041847  0.035198    -0.001315  1.000000 -0.029123   \n",
       "fc                  0.033334  0.003593    -0.000434 -0.029123  1.000000   \n",
       "four_g              0.015665  0.013443    -0.043073  0.003187 -0.016560   \n",
       "int_memory         -0.004004  0.041177     0.006545 -0.015679 -0.029133   \n",
       "m_dep               0.034085  0.004049    -0.014364 -0.022142 -0.001791   \n",
       "mobile_wt           0.001844 -0.008605     0.012350 -0.008979  0.023618   \n",
       "n_cores            -0.029727  0.036161    -0.005724 -0.024658 -0.013356   \n",
       "pc                  0.031441 -0.009952    -0.005245 -0.017143  0.644595   \n",
       "px_height           0.014901 -0.006872    -0.014523 -0.020875 -0.009990   \n",
       "px_width           -0.008402 -0.041533    -0.009476  0.014291 -0.005176   \n",
       "ram                -0.000653  0.026351     0.003443  0.041072  0.015099   \n",
       "sc_h               -0.029959 -0.002952    -0.029078 -0.011949 -0.011014   \n",
       "sc_w               -0.021421  0.000613    -0.007378 -0.016666 -0.012373   \n",
       "talk_time           0.052510  0.013934    -0.011432 -0.039404 -0.006829   \n",
       "three_g             0.011522 -0.030236    -0.046433 -0.014008  0.001793   \n",
       "touch_screen       -0.010516  0.010061     0.019756 -0.017117 -0.014828   \n",
       "wifi               -0.008343 -0.021863    -0.024471  0.022740  0.020085   \n",
       "price_range         0.200723  0.020573    -0.006606  0.017444  0.021998   \n",
       "\n",
       "                 four_g  int_memory     m_dep  mobile_wt   n_cores  ...  \\\n",
       "battery_power  0.015665   -0.004004  0.034085   0.001844 -0.029727  ...   \n",
       "blue           0.013443    0.041177  0.004049  -0.008605  0.036161  ...   \n",
       "clock_speed   -0.043073    0.006545 -0.014364   0.012350 -0.005724  ...   \n",
       "dual_sim       0.003187   -0.015679 -0.022142  -0.008979 -0.024658  ...   \n",
       "fc            -0.016560   -0.029133 -0.001791   0.023618 -0.013356  ...   \n",
       "four_g         1.000000    0.008690 -0.001823  -0.016537 -0.029706  ...   \n",
       "int_memory     0.008690    1.000000  0.006886  -0.034214 -0.028310  ...   \n",
       "m_dep         -0.001823    0.006886  1.000000   0.021756 -0.003504  ...   \n",
       "mobile_wt     -0.016537   -0.034214  0.021756   1.000000 -0.018989  ...   \n",
       "n_cores       -0.029706   -0.028310 -0.003504  -0.018989  1.000000  ...   \n",
       "pc            -0.005598   -0.033273  0.026282   0.018844 -0.001193  ...   \n",
       "px_height     -0.019236    0.010441  0.025263   0.000939 -0.006872  ...   \n",
       "px_width       0.007448   -0.008335  0.023566   0.000090  0.024480  ...   \n",
       "ram            0.007313    0.032813 -0.009434  -0.002581  0.004868  ...   \n",
       "sc_h           0.027166    0.037771 -0.025348  -0.033855 -0.000315  ...   \n",
       "sc_w           0.037005    0.011731 -0.018388  -0.020761  0.025826  ...   \n",
       "talk_time     -0.046628   -0.002790  0.017003   0.006209  0.013148  ...   \n",
       "three_g        0.584246   -0.009366 -0.012065   0.001551 -0.014733  ...   \n",
       "touch_screen   0.016758   -0.026999 -0.002638  -0.014368  0.023774  ...   \n",
       "wifi          -0.017620    0.006993 -0.028353  -0.000409 -0.009964  ...   \n",
       "price_range    0.014772    0.044435  0.000853  -0.030302  0.004399  ...   \n",
       "\n",
       "               px_height  px_width       ram      sc_h      sc_w  talk_time  \\\n",
       "battery_power   0.014901 -0.008402 -0.000653 -0.029959 -0.021421   0.052510   \n",
       "blue           -0.006872 -0.041533  0.026351 -0.002952  0.000613   0.013934   \n",
       "clock_speed    -0.014523 -0.009476  0.003443 -0.029078 -0.007378  -0.011432   \n",
       "dual_sim       -0.020875  0.014291  0.041072 -0.011949 -0.016666  -0.039404   \n",
       "fc             -0.009990 -0.005176  0.015099 -0.011014 -0.012373  -0.006829   \n",
       "four_g         -0.019236  0.007448  0.007313  0.027166  0.037005  -0.046628   \n",
       "int_memory      0.010441 -0.008335  0.032813  0.037771  0.011731  -0.002790   \n",
       "m_dep           0.025263  0.023566 -0.009434 -0.025348 -0.018388   0.017003   \n",
       "mobile_wt       0.000939  0.000090 -0.002581 -0.033855 -0.020761   0.006209   \n",
       "n_cores        -0.006872  0.024480  0.004868 -0.000315  0.025826   0.013148   \n",
       "pc             -0.018465  0.004196  0.028984  0.004938 -0.023819   0.014657   \n",
       "px_height       1.000000  0.510664 -0.020352  0.059615  0.043038  -0.010645   \n",
       "px_width        0.510664  1.000000  0.004105  0.021599  0.034699   0.006720   \n",
       "ram            -0.020352  0.004105  1.000000  0.015996  0.035576   0.010820   \n",
       "sc_h            0.059615  0.021599  0.015996  1.000000  0.506144  -0.017335   \n",
       "sc_w            0.043038  0.034699  0.035576  0.506144  1.000000  -0.022821   \n",
       "talk_time      -0.010645  0.006720  0.010820 -0.017335 -0.022821   1.000000   \n",
       "three_g        -0.031174  0.000350  0.015795  0.012033  0.030941  -0.042688   \n",
       "touch_screen    0.021891 -0.001628 -0.030455 -0.020023  0.012720   0.017196   \n",
       "wifi            0.051824  0.030319  0.022669  0.025929  0.035423  -0.029504   \n",
       "price_range     0.148858  0.165818  0.917046  0.022986  0.038711   0.021859   \n",
       "\n",
       "                three_g  touch_screen      wifi  price_range  \n",
       "battery_power  0.011522     -0.010516 -0.008343     0.200723  \n",
       "blue          -0.030236      0.010061 -0.021863     0.020573  \n",
       "clock_speed   -0.046433      0.019756 -0.024471    -0.006606  \n",
       "dual_sim      -0.014008     -0.017117  0.022740     0.017444  \n",
       "fc             0.001793     -0.014828  0.020085     0.021998  \n",
       "four_g         0.584246      0.016758 -0.017620     0.014772  \n",
       "int_memory    -0.009366     -0.026999  0.006993     0.044435  \n",
       "m_dep         -0.012065     -0.002638 -0.028353     0.000853  \n",
       "mobile_wt      0.001551     -0.014368 -0.000409    -0.030302  \n",
       "n_cores       -0.014733      0.023774 -0.009964     0.004399  \n",
       "pc            -0.001322     -0.008742  0.005389     0.033599  \n",
       "px_height     -0.031174      0.021891  0.051824     0.148858  \n",
       "px_width       0.000350     -0.001628  0.030319     0.165818  \n",
       "ram            0.015795     -0.030455  0.022669     0.917046  \n",
       "sc_h           0.012033     -0.020023  0.025929     0.022986  \n",
       "sc_w           0.030941      0.012720  0.035423     0.038711  \n",
       "talk_time     -0.042688      0.017196 -0.029504     0.021859  \n",
       "three_g        1.000000      0.013917  0.004316     0.023611  \n",
       "touch_screen   0.013917      1.000000  0.011917    -0.030411  \n",
       "wifi           0.004316      0.011917  1.000000     0.018785  \n",
       "price_range    0.023611     -0.030411  0.018785     1.000000  \n",
       "\n",
       "[21 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frame.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= train_frame.iloc[:, 0:20]\n",
    "y = train_frame.iloc[:,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=236)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "sc = StandardScaler().fit(X_train)\n",
    "X_train=sc.transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "enc = OneHotEncoder(categorical_features=[0])\n",
    "y_train= enc.fit_transform(np.expand_dims(y_train.values, axis=1))\n",
    "y_test =enc.fit_transform(np.expand_dims(y_test.values, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/300\n",
      "1600/1600 [==============================] - 1s 363us/step - loss: 1.6735 - accuracy: 0.2481 - val_loss: 1.3542 - val_accuracy: 0.3275\n",
      "Epoch 2/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 1.4890 - accuracy: 0.2656 - val_loss: 1.3601 - val_accuracy: 0.3725\n",
      "Epoch 3/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 1.3972 - accuracy: 0.2975 - val_loss: 1.3575 - val_accuracy: 0.3825\n",
      "Epoch 4/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 1.3830 - accuracy: 0.3081 - val_loss: 1.3558 - val_accuracy: 0.3850\n",
      "Epoch 5/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 1.3663 - accuracy: 0.2969 - val_loss: 1.3402 - val_accuracy: 0.4275\n",
      "Epoch 6/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 1.3428 - accuracy: 0.3237 - val_loss: 1.3193 - val_accuracy: 0.4775\n",
      "Epoch 7/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 1.3280 - accuracy: 0.3500 - val_loss: 1.2819 - val_accuracy: 0.5425\n",
      "Epoch 8/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 1.3093 - accuracy: 0.3531 - val_loss: 1.2379 - val_accuracy: 0.5900\n",
      "Epoch 9/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 1.2649 - accuracy: 0.3881 - val_loss: 1.1816 - val_accuracy: 0.6050\n",
      "Epoch 10/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 1.2645 - accuracy: 0.3769 - val_loss: 1.1338 - val_accuracy: 0.6450\n",
      "Epoch 11/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 1.2437 - accuracy: 0.3981 - val_loss: 1.0895 - val_accuracy: 0.6525\n",
      "Epoch 12/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 1.1977 - accuracy: 0.4119 - val_loss: 1.0383 - val_accuracy: 0.6675\n",
      "Epoch 13/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 1.1517 - accuracy: 0.4356 - val_loss: 0.9931 - val_accuracy: 0.6700\n",
      "Epoch 14/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 1.1504 - accuracy: 0.4444 - val_loss: 0.9524 - val_accuracy: 0.6975\n",
      "Epoch 15/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 1.1078 - accuracy: 0.4650 - val_loss: 0.9013 - val_accuracy: 0.6750\n",
      "Epoch 16/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 1.1032 - accuracy: 0.4737 - val_loss: 0.8709 - val_accuracy: 0.6725\n",
      "Epoch 17/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 1.0424 - accuracy: 0.5013 - val_loss: 0.8310 - val_accuracy: 0.6900\n",
      "Epoch 18/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 1.0034 - accuracy: 0.5281 - val_loss: 0.7868 - val_accuracy: 0.6925\n",
      "Epoch 19/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.9875 - accuracy: 0.5163 - val_loss: 0.7618 - val_accuracy: 0.7275\n",
      "Epoch 20/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.9643 - accuracy: 0.5606 - val_loss: 0.7290 - val_accuracy: 0.7150\n",
      "Epoch 21/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.9282 - accuracy: 0.5544 - val_loss: 0.7008 - val_accuracy: 0.7325\n",
      "Epoch 22/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.9187 - accuracy: 0.5619 - val_loss: 0.6802 - val_accuracy: 0.7450\n",
      "Epoch 23/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.8759 - accuracy: 0.5844 - val_loss: 0.6564 - val_accuracy: 0.7675\n",
      "Epoch 24/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.9029 - accuracy: 0.5713 - val_loss: 0.6465 - val_accuracy: 0.7750\n",
      "Epoch 25/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.8688 - accuracy: 0.5838 - val_loss: 0.6403 - val_accuracy: 0.8100\n",
      "Epoch 26/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.8394 - accuracy: 0.6106 - val_loss: 0.6211 - val_accuracy: 0.8050\n",
      "Epoch 27/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.7826 - accuracy: 0.6375 - val_loss: 0.5980 - val_accuracy: 0.7775\n",
      "Epoch 28/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.7986 - accuracy: 0.6300 - val_loss: 0.5894 - val_accuracy: 0.8250\n",
      "Epoch 29/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.7664 - accuracy: 0.6569 - val_loss: 0.5771 - val_accuracy: 0.8450\n",
      "Epoch 30/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.7710 - accuracy: 0.6494 - val_loss: 0.5638 - val_accuracy: 0.8425\n",
      "Epoch 31/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.7353 - accuracy: 0.6687 - val_loss: 0.5652 - val_accuracy: 0.8950\n",
      "Epoch 32/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.7675 - accuracy: 0.6406 - val_loss: 0.5513 - val_accuracy: 0.8675\n",
      "Epoch 33/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.7445 - accuracy: 0.6619 - val_loss: 0.5559 - val_accuracy: 0.9025\n",
      "Epoch 34/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.7375 - accuracy: 0.6569 - val_loss: 0.5522 - val_accuracy: 0.9225\n",
      "Epoch 35/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.7358 - accuracy: 0.6737 - val_loss: 0.5348 - val_accuracy: 0.8925\n",
      "Epoch 36/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.7179 - accuracy: 0.6656 - val_loss: 0.5284 - val_accuracy: 0.8825\n",
      "Epoch 37/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.7210 - accuracy: 0.6644 - val_loss: 0.5176 - val_accuracy: 0.8975\n",
      "Epoch 38/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.6943 - accuracy: 0.7025 - val_loss: 0.5068 - val_accuracy: 0.9175\n",
      "Epoch 39/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.6979 - accuracy: 0.6919 - val_loss: 0.5050 - val_accuracy: 0.9200\n",
      "Epoch 40/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.6716 - accuracy: 0.6956 - val_loss: 0.4906 - val_accuracy: 0.8925\n",
      "Epoch 41/300\n",
      "1600/1600 [==============================] - 0s 82us/step - loss: 0.6734 - accuracy: 0.7219 - val_loss: 0.4755 - val_accuracy: 0.8950\n",
      "Epoch 42/300\n",
      "1600/1600 [==============================] - 0s 82us/step - loss: 0.6679 - accuracy: 0.7138 - val_loss: 0.4687 - val_accuracy: 0.9150\n",
      "Epoch 43/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.6677 - accuracy: 0.6875 - val_loss: 0.4623 - val_accuracy: 0.9050\n",
      "Epoch 44/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.6751 - accuracy: 0.7063 - val_loss: 0.4551 - val_accuracy: 0.9150\n",
      "Epoch 45/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.6509 - accuracy: 0.7169 - val_loss: 0.4461 - val_accuracy: 0.9025\n",
      "Epoch 46/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.6470 - accuracy: 0.7394 - val_loss: 0.4481 - val_accuracy: 0.9175\n",
      "Epoch 47/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.6653 - accuracy: 0.7163 - val_loss: 0.4456 - val_accuracy: 0.9075\n",
      "Epoch 48/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.6319 - accuracy: 0.7281 - val_loss: 0.4382 - val_accuracy: 0.9075\n",
      "Epoch 49/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.6337 - accuracy: 0.7294 - val_loss: 0.4366 - val_accuracy: 0.9350\n",
      "Epoch 50/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.6093 - accuracy: 0.7381 - val_loss: 0.4220 - val_accuracy: 0.8950\n",
      "Epoch 51/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.6105 - accuracy: 0.7462 - val_loss: 0.4222 - val_accuracy: 0.9100\n",
      "Epoch 52/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.6101 - accuracy: 0.7412 - val_loss: 0.4181 - val_accuracy: 0.9075\n",
      "Epoch 53/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.6047 - accuracy: 0.7475 - val_loss: 0.4192 - val_accuracy: 0.9300\n",
      "Epoch 54/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.5921 - accuracy: 0.7556 - val_loss: 0.4068 - val_accuracy: 0.9000\n",
      "Epoch 55/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.6072 - accuracy: 0.7531 - val_loss: 0.4067 - val_accuracy: 0.9175\n",
      "Epoch 56/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.5807 - accuracy: 0.7606 - val_loss: 0.4001 - val_accuracy: 0.9050\n",
      "Epoch 57/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.5776 - accuracy: 0.7588 - val_loss: 0.3901 - val_accuracy: 0.9100\n",
      "Epoch 58/300\n",
      "1600/1600 [==============================] - 0s 93us/step - loss: 0.6089 - accuracy: 0.7462 - val_loss: 0.3896 - val_accuracy: 0.9200\n",
      "Epoch 59/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.6233 - accuracy: 0.7387 - val_loss: 0.3872 - val_accuracy: 0.9025\n",
      "Epoch 60/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.5823 - accuracy: 0.7638 - val_loss: 0.3885 - val_accuracy: 0.9175\n",
      "Epoch 61/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.5543 - accuracy: 0.7781 - val_loss: 0.3776 - val_accuracy: 0.8900\n",
      "Epoch 62/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.5809 - accuracy: 0.7475 - val_loss: 0.3816 - val_accuracy: 0.9225\n",
      "Epoch 63/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.5936 - accuracy: 0.7400 - val_loss: 0.3773 - val_accuracy: 0.9125\n",
      "Epoch 64/300\n",
      "1600/1600 [==============================] - 0s 74us/step - loss: 0.5500 - accuracy: 0.7744 - val_loss: 0.3804 - val_accuracy: 0.9200\n",
      "Epoch 65/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.5642 - accuracy: 0.7769 - val_loss: 0.3699 - val_accuracy: 0.9100\n",
      "Epoch 66/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.5446 - accuracy: 0.7769 - val_loss: 0.3664 - val_accuracy: 0.9125\n",
      "Epoch 67/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.5329 - accuracy: 0.7806 - val_loss: 0.3669 - val_accuracy: 0.9175\n",
      "Epoch 68/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.5358 - accuracy: 0.7812 - val_loss: 0.3639 - val_accuracy: 0.9100\n",
      "Epoch 69/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.5560 - accuracy: 0.7738 - val_loss: 0.3564 - val_accuracy: 0.9050\n",
      "Epoch 70/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.5534 - accuracy: 0.7725 - val_loss: 0.3601 - val_accuracy: 0.9175\n",
      "Epoch 71/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.5567 - accuracy: 0.7725 - val_loss: 0.3494 - val_accuracy: 0.9050\n",
      "Epoch 72/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.5182 - accuracy: 0.7819 - val_loss: 0.3481 - val_accuracy: 0.9175\n",
      "Epoch 73/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.5196 - accuracy: 0.7800 - val_loss: 0.3398 - val_accuracy: 0.8950\n",
      "Epoch 74/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.5337 - accuracy: 0.7800 - val_loss: 0.3378 - val_accuracy: 0.9025\n",
      "Epoch 75/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.5159 - accuracy: 0.7981 - val_loss: 0.3382 - val_accuracy: 0.9125\n",
      "Epoch 76/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.5119 - accuracy: 0.7981 - val_loss: 0.3372 - val_accuracy: 0.9200\n",
      "Epoch 77/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4784 - accuracy: 0.8012 - val_loss: 0.3375 - val_accuracy: 0.9175\n",
      "Epoch 78/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.5010 - accuracy: 0.8000 - val_loss: 0.3259 - val_accuracy: 0.9075\n",
      "Epoch 79/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.5228 - accuracy: 0.7906 - val_loss: 0.3249 - val_accuracy: 0.9075\n",
      "Epoch 80/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4959 - accuracy: 0.8050 - val_loss: 0.3255 - val_accuracy: 0.9050\n",
      "Epoch 81/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.4933 - accuracy: 0.8062 - val_loss: 0.3294 - val_accuracy: 0.9350\n",
      "Epoch 82/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.5084 - accuracy: 0.8006 - val_loss: 0.3167 - val_accuracy: 0.9225\n",
      "Epoch 83/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4881 - accuracy: 0.8037 - val_loss: 0.3072 - val_accuracy: 0.9225\n",
      "Epoch 84/300\n",
      "1600/1600 [==============================] - 0s 83us/step - loss: 0.4709 - accuracy: 0.8106 - val_loss: 0.3023 - val_accuracy: 0.9000\n",
      "Epoch 85/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4755 - accuracy: 0.8119 - val_loss: 0.3007 - val_accuracy: 0.8950\n",
      "Epoch 86/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.5009 - accuracy: 0.7969 - val_loss: 0.3013 - val_accuracy: 0.9350\n",
      "Epoch 87/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.4652 - accuracy: 0.8194 - val_loss: 0.2954 - val_accuracy: 0.9250\n",
      "Epoch 88/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4640 - accuracy: 0.8206 - val_loss: 0.2926 - val_accuracy: 0.9250\n",
      "Epoch 89/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4952 - accuracy: 0.8019 - val_loss: 0.2904 - val_accuracy: 0.9325\n",
      "Epoch 90/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.4721 - accuracy: 0.8206 - val_loss: 0.3060 - val_accuracy: 0.9475\n",
      "Epoch 91/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4713 - accuracy: 0.8163 - val_loss: 0.2962 - val_accuracy: 0.9475\n",
      "Epoch 92/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4690 - accuracy: 0.8119 - val_loss: 0.2863 - val_accuracy: 0.9350\n",
      "Epoch 93/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4770 - accuracy: 0.8025 - val_loss: 0.2834 - val_accuracy: 0.9225\n",
      "Epoch 94/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4617 - accuracy: 0.8219 - val_loss: 0.2824 - val_accuracy: 0.9375\n",
      "Epoch 95/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.4637 - accuracy: 0.8188 - val_loss: 0.2774 - val_accuracy: 0.9325\n",
      "Epoch 96/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4581 - accuracy: 0.8256 - val_loss: 0.2822 - val_accuracy: 0.9325\n",
      "Epoch 97/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4207 - accuracy: 0.8338 - val_loss: 0.2888 - val_accuracy: 0.9350\n",
      "Epoch 98/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.4491 - accuracy: 0.8200 - val_loss: 0.2704 - val_accuracy: 0.9275\n",
      "Epoch 99/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4509 - accuracy: 0.8275 - val_loss: 0.2760 - val_accuracy: 0.9300\n",
      "Epoch 100/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.4507 - accuracy: 0.8256 - val_loss: 0.2679 - val_accuracy: 0.9275\n",
      "Epoch 101/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.4242 - accuracy: 0.8238 - val_loss: 0.2681 - val_accuracy: 0.9225\n",
      "Epoch 102/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.4638 - accuracy: 0.8106 - val_loss: 0.2706 - val_accuracy: 0.9325\n",
      "Epoch 103/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4396 - accuracy: 0.8200 - val_loss: 0.2704 - val_accuracy: 0.9325\n",
      "Epoch 104/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.4177 - accuracy: 0.8456 - val_loss: 0.2757 - val_accuracy: 0.9375\n",
      "Epoch 105/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4335 - accuracy: 0.8350 - val_loss: 0.2845 - val_accuracy: 0.9400\n",
      "Epoch 106/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4392 - accuracy: 0.8244 - val_loss: 0.2707 - val_accuracy: 0.9300\n",
      "Epoch 107/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4139 - accuracy: 0.8356 - val_loss: 0.2761 - val_accuracy: 0.9025\n",
      "Epoch 108/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4315 - accuracy: 0.8375 - val_loss: 0.2765 - val_accuracy: 0.9375\n",
      "Epoch 109/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.4402 - accuracy: 0.8256 - val_loss: 0.2662 - val_accuracy: 0.9350\n",
      "Epoch 110/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.4372 - accuracy: 0.8363 - val_loss: 0.2647 - val_accuracy: 0.9350\n",
      "Epoch 111/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.4324 - accuracy: 0.8256 - val_loss: 0.2595 - val_accuracy: 0.9325\n",
      "Epoch 112/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.4308 - accuracy: 0.8350 - val_loss: 0.2622 - val_accuracy: 0.9350\n",
      "Epoch 113/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4310 - accuracy: 0.8369 - val_loss: 0.2525 - val_accuracy: 0.9375\n",
      "Epoch 114/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.4258 - accuracy: 0.8263 - val_loss: 0.2514 - val_accuracy: 0.9400\n",
      "Epoch 115/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4312 - accuracy: 0.8431 - val_loss: 0.2492 - val_accuracy: 0.9450\n",
      "Epoch 116/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4199 - accuracy: 0.8413 - val_loss: 0.2643 - val_accuracy: 0.9575\n",
      "Epoch 117/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4450 - accuracy: 0.8281 - val_loss: 0.2470 - val_accuracy: 0.9375\n",
      "Epoch 118/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4082 - accuracy: 0.8587 - val_loss: 0.2447 - val_accuracy: 0.9450\n",
      "Epoch 119/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3953 - accuracy: 0.8419 - val_loss: 0.2414 - val_accuracy: 0.9450\n",
      "Epoch 120/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4109 - accuracy: 0.8400 - val_loss: 0.2427 - val_accuracy: 0.9300\n",
      "Epoch 121/300\n",
      "1600/1600 [==============================] - 0s 82us/step - loss: 0.4173 - accuracy: 0.8425 - val_loss: 0.2442 - val_accuracy: 0.9475\n",
      "Epoch 122/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4335 - accuracy: 0.8394 - val_loss: 0.2463 - val_accuracy: 0.9450\n",
      "Epoch 123/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4122 - accuracy: 0.8388 - val_loss: 0.2444 - val_accuracy: 0.9425\n",
      "Epoch 124/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4304 - accuracy: 0.8313 - val_loss: 0.2453 - val_accuracy: 0.9325\n",
      "Epoch 125/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4133 - accuracy: 0.8413 - val_loss: 0.2461 - val_accuracy: 0.9425\n",
      "Epoch 126/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.4145 - accuracy: 0.8431 - val_loss: 0.2484 - val_accuracy: 0.9375\n",
      "Epoch 127/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.4122 - accuracy: 0.8400 - val_loss: 0.2456 - val_accuracy: 0.9425\n",
      "Epoch 128/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.4057 - accuracy: 0.8475 - val_loss: 0.2452 - val_accuracy: 0.9400\n",
      "Epoch 129/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.4121 - accuracy: 0.8500 - val_loss: 0.2406 - val_accuracy: 0.9400\n",
      "Epoch 130/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3841 - accuracy: 0.8544 - val_loss: 0.2379 - val_accuracy: 0.9325\n",
      "Epoch 131/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4010 - accuracy: 0.8450 - val_loss: 0.2424 - val_accuracy: 0.9525\n",
      "Epoch 132/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.4204 - accuracy: 0.8400 - val_loss: 0.2327 - val_accuracy: 0.9475\n",
      "Epoch 133/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.4076 - accuracy: 0.8431 - val_loss: 0.2344 - val_accuracy: 0.9400\n",
      "Epoch 134/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.3915 - accuracy: 0.8537 - val_loss: 0.2312 - val_accuracy: 0.9375\n",
      "Epoch 135/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3885 - accuracy: 0.8600 - val_loss: 0.2305 - val_accuracy: 0.9400\n",
      "Epoch 136/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3763 - accuracy: 0.8512 - val_loss: 0.2320 - val_accuracy: 0.9375\n",
      "Epoch 137/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.3840 - accuracy: 0.8594 - val_loss: 0.2317 - val_accuracy: 0.9425\n",
      "Epoch 138/300\n",
      "1600/1600 [==============================] - 0s 96us/step - loss: 0.3701 - accuracy: 0.8550 - val_loss: 0.2217 - val_accuracy: 0.9450\n",
      "Epoch 139/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.3644 - accuracy: 0.8581 - val_loss: 0.2223 - val_accuracy: 0.9425\n",
      "Epoch 140/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.3871 - accuracy: 0.8544 - val_loss: 0.2179 - val_accuracy: 0.9450\n",
      "Epoch 141/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3861 - accuracy: 0.8506 - val_loss: 0.2227 - val_accuracy: 0.9500\n",
      "Epoch 142/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.3933 - accuracy: 0.8531 - val_loss: 0.2180 - val_accuracy: 0.9500\n",
      "Epoch 143/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3783 - accuracy: 0.8594 - val_loss: 0.2260 - val_accuracy: 0.9325\n",
      "Epoch 144/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3725 - accuracy: 0.8519 - val_loss: 0.2118 - val_accuracy: 0.9500\n",
      "Epoch 145/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3677 - accuracy: 0.8600 - val_loss: 0.2050 - val_accuracy: 0.9500\n",
      "Epoch 146/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3719 - accuracy: 0.8556 - val_loss: 0.2211 - val_accuracy: 0.9450\n",
      "Epoch 147/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3780 - accuracy: 0.8650 - val_loss: 0.2141 - val_accuracy: 0.9400\n",
      "Epoch 148/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.3667 - accuracy: 0.8625 - val_loss: 0.2195 - val_accuracy: 0.9375\n",
      "Epoch 149/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.3834 - accuracy: 0.8587 - val_loss: 0.2159 - val_accuracy: 0.9400\n",
      "Epoch 150/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3700 - accuracy: 0.8662 - val_loss: 0.2170 - val_accuracy: 0.9400\n",
      "Epoch 151/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3623 - accuracy: 0.8569 - val_loss: 0.2091 - val_accuracy: 0.9450\n",
      "Epoch 152/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3913 - accuracy: 0.8550 - val_loss: 0.2021 - val_accuracy: 0.9450\n",
      "Epoch 153/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.3906 - accuracy: 0.8569 - val_loss: 0.1955 - val_accuracy: 0.9475\n",
      "Epoch 154/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3575 - accuracy: 0.8644 - val_loss: 0.1956 - val_accuracy: 0.9450\n",
      "Epoch 155/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3897 - accuracy: 0.8556 - val_loss: 0.1966 - val_accuracy: 0.9425\n",
      "Epoch 156/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.3667 - accuracy: 0.8594 - val_loss: 0.1943 - val_accuracy: 0.9400\n",
      "Epoch 157/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3440 - accuracy: 0.8788 - val_loss: 0.1956 - val_accuracy: 0.9425\n",
      "Epoch 158/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.3653 - accuracy: 0.8631 - val_loss: 0.1927 - val_accuracy: 0.9450\n",
      "Epoch 159/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3809 - accuracy: 0.8462 - val_loss: 0.1921 - val_accuracy: 0.9475\n",
      "Epoch 160/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3553 - accuracy: 0.8619 - val_loss: 0.1834 - val_accuracy: 0.9425\n",
      "Epoch 161/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.3698 - accuracy: 0.8744 - val_loss: 0.1904 - val_accuracy: 0.9600\n",
      "Epoch 162/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3658 - accuracy: 0.8763 - val_loss: 0.1820 - val_accuracy: 0.9525\n",
      "Epoch 163/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3598 - accuracy: 0.8556 - val_loss: 0.1852 - val_accuracy: 0.9600\n",
      "Epoch 164/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.3755 - accuracy: 0.8637 - val_loss: 0.1810 - val_accuracy: 0.9475\n",
      "Epoch 165/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3458 - accuracy: 0.8700 - val_loss: 0.1881 - val_accuracy: 0.9450\n",
      "Epoch 166/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3496 - accuracy: 0.8687 - val_loss: 0.1805 - val_accuracy: 0.9500\n",
      "Epoch 167/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.3179 - accuracy: 0.8869 - val_loss: 0.1829 - val_accuracy: 0.9550\n",
      "Epoch 168/300\n",
      "1600/1600 [==============================] - 0s 82us/step - loss: 0.3758 - accuracy: 0.8594 - val_loss: 0.1812 - val_accuracy: 0.9625\n",
      "Epoch 169/300\n",
      "1600/1600 [==============================] - 0s 83us/step - loss: 0.3339 - accuracy: 0.8763 - val_loss: 0.1754 - val_accuracy: 0.9550\n",
      "Epoch 170/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.3573 - accuracy: 0.8644 - val_loss: 0.1757 - val_accuracy: 0.9475\n",
      "Epoch 171/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.3560 - accuracy: 0.8656 - val_loss: 0.1714 - val_accuracy: 0.9575\n",
      "Epoch 172/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3547 - accuracy: 0.8669 - val_loss: 0.1742 - val_accuracy: 0.9525\n",
      "Epoch 173/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.3369 - accuracy: 0.8850 - val_loss: 0.1722 - val_accuracy: 0.9525\n",
      "Epoch 174/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3556 - accuracy: 0.8781 - val_loss: 0.1708 - val_accuracy: 0.9550\n",
      "Epoch 175/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3537 - accuracy: 0.8700 - val_loss: 0.1747 - val_accuracy: 0.9550\n",
      "Epoch 176/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.3664 - accuracy: 0.8744 - val_loss: 0.1682 - val_accuracy: 0.9575\n",
      "Epoch 177/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.2952 - accuracy: 0.8925 - val_loss: 0.1673 - val_accuracy: 0.9550\n",
      "Epoch 178/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.3175 - accuracy: 0.8800 - val_loss: 0.1652 - val_accuracy: 0.9575\n",
      "Epoch 179/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3472 - accuracy: 0.8662 - val_loss: 0.1724 - val_accuracy: 0.9550\n",
      "Epoch 180/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3446 - accuracy: 0.8731 - val_loss: 0.1638 - val_accuracy: 0.9575\n",
      "Epoch 181/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3423 - accuracy: 0.8737 - val_loss: 0.1611 - val_accuracy: 0.9575\n",
      "Epoch 182/300\n",
      "1600/1600 [==============================] - 0s 82us/step - loss: 0.3493 - accuracy: 0.8800 - val_loss: 0.1595 - val_accuracy: 0.9600\n",
      "Epoch 183/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3187 - accuracy: 0.8775 - val_loss: 0.1602 - val_accuracy: 0.9575\n",
      "Epoch 184/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.3389 - accuracy: 0.8731 - val_loss: 0.1611 - val_accuracy: 0.9575\n",
      "Epoch 185/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3359 - accuracy: 0.8712 - val_loss: 0.1607 - val_accuracy: 0.9600\n",
      "Epoch 186/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.3351 - accuracy: 0.8744 - val_loss: 0.1520 - val_accuracy: 0.9575\n",
      "Epoch 187/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.3113 - accuracy: 0.8825 - val_loss: 0.1556 - val_accuracy: 0.9575\n",
      "Epoch 188/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3394 - accuracy: 0.8744 - val_loss: 0.1523 - val_accuracy: 0.9675\n",
      "Epoch 189/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3423 - accuracy: 0.8813 - val_loss: 0.1520 - val_accuracy: 0.9600\n",
      "Epoch 190/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.3076 - accuracy: 0.8863 - val_loss: 0.1432 - val_accuracy: 0.9675\n",
      "Epoch 191/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3225 - accuracy: 0.8756 - val_loss: 0.1511 - val_accuracy: 0.9625\n",
      "Epoch 192/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3288 - accuracy: 0.8850 - val_loss: 0.1456 - val_accuracy: 0.9650\n",
      "Epoch 193/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.3134 - accuracy: 0.8881 - val_loss: 0.1408 - val_accuracy: 0.9625\n",
      "Epoch 194/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3220 - accuracy: 0.8806 - val_loss: 0.1413 - val_accuracy: 0.9550\n",
      "Epoch 195/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.3159 - accuracy: 0.8844 - val_loss: 0.1478 - val_accuracy: 0.9600\n",
      "Epoch 196/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.3181 - accuracy: 0.8800 - val_loss: 0.1416 - val_accuracy: 0.9625\n",
      "Epoch 197/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.3312 - accuracy: 0.8844 - val_loss: 0.1436 - val_accuracy: 0.9650\n",
      "Epoch 198/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2778 - accuracy: 0.9062 - val_loss: 0.1436 - val_accuracy: 0.9600\n",
      "Epoch 199/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.3487 - accuracy: 0.8744 - val_loss: 0.1407 - val_accuracy: 0.9600\n",
      "Epoch 200/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3119 - accuracy: 0.8831 - val_loss: 0.1427 - val_accuracy: 0.9650\n",
      "Epoch 201/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.3090 - accuracy: 0.8938 - val_loss: 0.1451 - val_accuracy: 0.9600\n",
      "Epoch 202/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.3148 - accuracy: 0.8925 - val_loss: 0.1463 - val_accuracy: 0.9550\n",
      "Epoch 203/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3286 - accuracy: 0.8819 - val_loss: 0.1472 - val_accuracy: 0.9575\n",
      "Epoch 204/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3221 - accuracy: 0.8756 - val_loss: 0.1417 - val_accuracy: 0.9625\n",
      "Epoch 205/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3182 - accuracy: 0.8775 - val_loss: 0.1452 - val_accuracy: 0.9575\n",
      "Epoch 206/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3391 - accuracy: 0.8825 - val_loss: 0.1380 - val_accuracy: 0.9675\n",
      "Epoch 207/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.3040 - accuracy: 0.8900 - val_loss: 0.1348 - val_accuracy: 0.9625\n",
      "Epoch 208/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3085 - accuracy: 0.8831 - val_loss: 0.1341 - val_accuracy: 0.9600\n",
      "Epoch 209/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3222 - accuracy: 0.8863 - val_loss: 0.1387 - val_accuracy: 0.9600\n",
      "Epoch 210/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3458 - accuracy: 0.8662 - val_loss: 0.1451 - val_accuracy: 0.9600\n",
      "Epoch 211/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2983 - accuracy: 0.8925 - val_loss: 0.1439 - val_accuracy: 0.9575\n",
      "Epoch 212/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.3009 - accuracy: 0.8894 - val_loss: 0.1378 - val_accuracy: 0.9575\n",
      "Epoch 213/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.2934 - accuracy: 0.8938 - val_loss: 0.1386 - val_accuracy: 0.9550\n",
      "Epoch 214/300\n",
      "1600/1600 [==============================] - 0s 82us/step - loss: 0.3192 - accuracy: 0.8794 - val_loss: 0.1422 - val_accuracy: 0.9525\n",
      "Epoch 215/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3172 - accuracy: 0.8869 - val_loss: 0.1341 - val_accuracy: 0.9600\n",
      "Epoch 216/300\n",
      "1600/1600 [==============================] - 0s 90us/step - loss: 0.3040 - accuracy: 0.8850 - val_loss: 0.1329 - val_accuracy: 0.9600\n",
      "Epoch 217/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.2850 - accuracy: 0.8938 - val_loss: 0.1345 - val_accuracy: 0.9625\n",
      "Epoch 218/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3068 - accuracy: 0.8844 - val_loss: 0.1408 - val_accuracy: 0.9525\n",
      "Epoch 219/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3058 - accuracy: 0.8869 - val_loss: 0.1347 - val_accuracy: 0.9550\n",
      "Epoch 220/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2751 - accuracy: 0.8944 - val_loss: 0.1275 - val_accuracy: 0.9675\n",
      "Epoch 221/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3326 - accuracy: 0.8825 - val_loss: 0.1362 - val_accuracy: 0.9600\n",
      "Epoch 222/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.2877 - accuracy: 0.8856 - val_loss: 0.1367 - val_accuracy: 0.9575\n",
      "Epoch 223/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3007 - accuracy: 0.8856 - val_loss: 0.1430 - val_accuracy: 0.9550\n",
      "Epoch 224/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.2926 - accuracy: 0.8919 - val_loss: 0.1388 - val_accuracy: 0.9550\n",
      "Epoch 225/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.3146 - accuracy: 0.8863 - val_loss: 0.1360 - val_accuracy: 0.9550\n",
      "Epoch 226/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.2765 - accuracy: 0.9000 - val_loss: 0.1295 - val_accuracy: 0.9525\n",
      "Epoch 227/300\n",
      "1600/1600 [==============================] - 0s 93us/step - loss: 0.2829 - accuracy: 0.8969 - val_loss: 0.1220 - val_accuracy: 0.9600\n",
      "Epoch 228/300\n",
      "1600/1600 [==============================] - 0s 82us/step - loss: 0.2662 - accuracy: 0.8950 - val_loss: 0.1273 - val_accuracy: 0.9550\n",
      "Epoch 229/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.2984 - accuracy: 0.8913 - val_loss: 0.1286 - val_accuracy: 0.9575\n",
      "Epoch 230/300\n",
      "1600/1600 [==============================] - 0s 88us/step - loss: 0.2986 - accuracy: 0.8850 - val_loss: 0.1235 - val_accuracy: 0.9675\n",
      "Epoch 231/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2893 - accuracy: 0.8881 - val_loss: 0.1213 - val_accuracy: 0.9675\n",
      "Epoch 232/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3063 - accuracy: 0.8881 - val_loss: 0.1322 - val_accuracy: 0.9525\n",
      "Epoch 233/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.2961 - accuracy: 0.8850 - val_loss: 0.1306 - val_accuracy: 0.9550\n",
      "Epoch 234/300\n",
      "1600/1600 [==============================] - 0s 102us/step - loss: 0.2990 - accuracy: 0.8838 - val_loss: 0.1288 - val_accuracy: 0.9575\n",
      "Epoch 235/300\n",
      "1600/1600 [==============================] - 0s 87us/step - loss: 0.3037 - accuracy: 0.8769 - val_loss: 0.1284 - val_accuracy: 0.9575\n",
      "Epoch 236/300\n",
      "1600/1600 [==============================] - 0s 84us/step - loss: 0.2949 - accuracy: 0.9038 - val_loss: 0.1266 - val_accuracy: 0.9575\n",
      "Epoch 237/300\n",
      "1600/1600 [==============================] - 0s 86us/step - loss: 0.3089 - accuracy: 0.8750 - val_loss: 0.1281 - val_accuracy: 0.9625\n",
      "Epoch 238/300\n",
      "1600/1600 [==============================] - 0s 83us/step - loss: 0.2964 - accuracy: 0.8925 - val_loss: 0.1251 - val_accuracy: 0.9625\n",
      "Epoch 239/300\n",
      "1600/1600 [==============================] - 0s 83us/step - loss: 0.2764 - accuracy: 0.8950 - val_loss: 0.1261 - val_accuracy: 0.9600\n",
      "Epoch 240/300\n",
      "1600/1600 [==============================] - 0s 83us/step - loss: 0.2563 - accuracy: 0.8975 - val_loss: 0.1264 - val_accuracy: 0.9600\n",
      "Epoch 241/300\n",
      "1600/1600 [==============================] - 0s 83us/step - loss: 0.3014 - accuracy: 0.8925 - val_loss: 0.1235 - val_accuracy: 0.9625\n",
      "Epoch 242/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.3042 - accuracy: 0.8781 - val_loss: 0.1219 - val_accuracy: 0.9625\n",
      "Epoch 243/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.2787 - accuracy: 0.8925 - val_loss: 0.1241 - val_accuracy: 0.9675\n",
      "Epoch 244/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2737 - accuracy: 0.9025 - val_loss: 0.1283 - val_accuracy: 0.9575\n",
      "Epoch 245/300\n",
      "1600/1600 [==============================] - 0s 87us/step - loss: 0.2977 - accuracy: 0.8813 - val_loss: 0.1308 - val_accuracy: 0.9550\n",
      "Epoch 246/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.2793 - accuracy: 0.8944 - val_loss: 0.1267 - val_accuracy: 0.9600\n",
      "Epoch 247/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.2989 - accuracy: 0.8875 - val_loss: 0.1232 - val_accuracy: 0.9600\n",
      "Epoch 248/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2496 - accuracy: 0.9050 - val_loss: 0.1213 - val_accuracy: 0.9575\n",
      "Epoch 249/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2711 - accuracy: 0.9006 - val_loss: 0.1196 - val_accuracy: 0.9575\n",
      "Epoch 250/300\n",
      "1600/1600 [==============================] - 0s 82us/step - loss: 0.2791 - accuracy: 0.9006 - val_loss: 0.1241 - val_accuracy: 0.9525\n",
      "Epoch 251/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.2625 - accuracy: 0.9000 - val_loss: 0.1207 - val_accuracy: 0.9575\n",
      "Epoch 252/300\n",
      "1600/1600 [==============================] - 0s 83us/step - loss: 0.2731 - accuracy: 0.8963 - val_loss: 0.1246 - val_accuracy: 0.9575\n",
      "Epoch 253/300\n",
      "1600/1600 [==============================] - 0s 82us/step - loss: 0.2801 - accuracy: 0.8850 - val_loss: 0.1175 - val_accuracy: 0.9625\n",
      "Epoch 254/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.2848 - accuracy: 0.8894 - val_loss: 0.1170 - val_accuracy: 0.9600\n",
      "Epoch 255/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.2814 - accuracy: 0.8944 - val_loss: 0.1152 - val_accuracy: 0.9675\n",
      "Epoch 256/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2517 - accuracy: 0.9025 - val_loss: 0.1147 - val_accuracy: 0.9650\n",
      "Epoch 257/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2894 - accuracy: 0.8906 - val_loss: 0.1210 - val_accuracy: 0.9625\n",
      "Epoch 258/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.2984 - accuracy: 0.8931 - val_loss: 0.1178 - val_accuracy: 0.9675\n",
      "Epoch 259/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.2699 - accuracy: 0.8956 - val_loss: 0.1182 - val_accuracy: 0.9650\n",
      "Epoch 260/300\n",
      "1600/1600 [==============================] - 0s 92us/step - loss: 0.2590 - accuracy: 0.9038 - val_loss: 0.1150 - val_accuracy: 0.9675\n",
      "Epoch 261/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2943 - accuracy: 0.8869 - val_loss: 0.1137 - val_accuracy: 0.9650\n",
      "Epoch 262/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.2750 - accuracy: 0.8925 - val_loss: 0.1218 - val_accuracy: 0.9600\n",
      "Epoch 263/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.3053 - accuracy: 0.8888 - val_loss: 0.1226 - val_accuracy: 0.9625\n",
      "Epoch 264/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.2774 - accuracy: 0.8894 - val_loss: 0.1193 - val_accuracy: 0.9725\n",
      "Epoch 265/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.2346 - accuracy: 0.9112 - val_loss: 0.1268 - val_accuracy: 0.9575\n",
      "Epoch 266/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.2802 - accuracy: 0.8988 - val_loss: 0.1261 - val_accuracy: 0.9600\n",
      "Epoch 267/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2487 - accuracy: 0.9025 - val_loss: 0.1262 - val_accuracy: 0.9600\n",
      "Epoch 268/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2662 - accuracy: 0.9031 - val_loss: 0.1219 - val_accuracy: 0.9650\n",
      "Epoch 269/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2584 - accuracy: 0.8931 - val_loss: 0.1290 - val_accuracy: 0.9525\n",
      "Epoch 270/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.2684 - accuracy: 0.8938 - val_loss: 0.1248 - val_accuracy: 0.9600\n",
      "Epoch 271/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.2702 - accuracy: 0.8975 - val_loss: 0.1218 - val_accuracy: 0.9600\n",
      "Epoch 272/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2355 - accuracy: 0.9100 - val_loss: 0.1172 - val_accuracy: 0.9650\n",
      "Epoch 273/300\n",
      "1600/1600 [==============================] - 0s 81us/step - loss: 0.2807 - accuracy: 0.8963 - val_loss: 0.1212 - val_accuracy: 0.9650\n",
      "Epoch 274/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2698 - accuracy: 0.8969 - val_loss: 0.1236 - val_accuracy: 0.9675\n",
      "Epoch 275/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2732 - accuracy: 0.8975 - val_loss: 0.1317 - val_accuracy: 0.9600\n",
      "Epoch 276/300\n",
      "1600/1600 [==============================] - 0s 97us/step - loss: 0.2787 - accuracy: 0.8919 - val_loss: 0.1238 - val_accuracy: 0.9600\n",
      "Epoch 277/300\n",
      "1600/1600 [==============================] - 0s 86us/step - loss: 0.2893 - accuracy: 0.8794 - val_loss: 0.1189 - val_accuracy: 0.9625\n",
      "Epoch 278/300\n",
      "1600/1600 [==============================] - 0s 84us/step - loss: 0.2780 - accuracy: 0.8950 - val_loss: 0.1223 - val_accuracy: 0.9625\n",
      "Epoch 279/300\n",
      "1600/1600 [==============================] - 0s 86us/step - loss: 0.2889 - accuracy: 0.8906 - val_loss: 0.1208 - val_accuracy: 0.9625\n",
      "Epoch 280/300\n",
      "1600/1600 [==============================] - 0s 83us/step - loss: 0.2787 - accuracy: 0.8913 - val_loss: 0.1218 - val_accuracy: 0.9650\n",
      "Epoch 281/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2997 - accuracy: 0.8850 - val_loss: 0.1232 - val_accuracy: 0.9600\n",
      "Epoch 282/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.3024 - accuracy: 0.8913 - val_loss: 0.1169 - val_accuracy: 0.9600\n",
      "Epoch 283/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2642 - accuracy: 0.8975 - val_loss: 0.1178 - val_accuracy: 0.9650\n",
      "Epoch 284/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.2686 - accuracy: 0.8988 - val_loss: 0.1181 - val_accuracy: 0.9650\n",
      "Epoch 285/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2926 - accuracy: 0.8881 - val_loss: 0.1227 - val_accuracy: 0.9600\n",
      "Epoch 286/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2827 - accuracy: 0.8956 - val_loss: 0.1200 - val_accuracy: 0.9650\n",
      "Epoch 287/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2852 - accuracy: 0.8881 - val_loss: 0.1198 - val_accuracy: 0.9650\n",
      "Epoch 288/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2902 - accuracy: 0.8944 - val_loss: 0.1147 - val_accuracy: 0.9650\n",
      "Epoch 289/300\n",
      "1600/1600 [==============================] - 0s 79us/step - loss: 0.2716 - accuracy: 0.8988 - val_loss: 0.1200 - val_accuracy: 0.9675\n",
      "Epoch 290/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2531 - accuracy: 0.9087 - val_loss: 0.1201 - val_accuracy: 0.9625\n",
      "Epoch 291/300\n",
      "1600/1600 [==============================] - 0s 87us/step - loss: 0.2563 - accuracy: 0.9081 - val_loss: 0.1165 - val_accuracy: 0.9625\n",
      "Epoch 292/300\n",
      "1600/1600 [==============================] - 0s 77us/step - loss: 0.2512 - accuracy: 0.9075 - val_loss: 0.1210 - val_accuracy: 0.9625\n",
      "Epoch 293/300\n",
      "1600/1600 [==============================] - 0s 91us/step - loss: 0.2748 - accuracy: 0.8931 - val_loss: 0.1187 - val_accuracy: 0.9625\n",
      "Epoch 294/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.2868 - accuracy: 0.8944 - val_loss: 0.1185 - val_accuracy: 0.9650\n",
      "Epoch 295/300\n",
      "1600/1600 [==============================] - 0s 74us/step - loss: 0.2564 - accuracy: 0.9031 - val_loss: 0.1220 - val_accuracy: 0.9650\n",
      "Epoch 296/300\n",
      "1600/1600 [==============================] - 0s 75us/step - loss: 0.2550 - accuracy: 0.9006 - val_loss: 0.1226 - val_accuracy: 0.9650\n",
      "Epoch 297/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2873 - accuracy: 0.8994 - val_loss: 0.1226 - val_accuracy: 0.9650\n",
      "Epoch 298/300\n",
      "1600/1600 [==============================] - 0s 76us/step - loss: 0.2460 - accuracy: 0.9200 - val_loss: 0.1199 - val_accuracy: 0.9650\n",
      "Epoch 299/300\n",
      "1600/1600 [==============================] - 0s 80us/step - loss: 0.2547 - accuracy: 0.9081 - val_loss: 0.1162 - val_accuracy: 0.9675\n",
      "Epoch 300/300\n",
      "1600/1600 [==============================] - 0s 78us/step - loss: 0.2798 - accuracy: 0.9094 - val_loss: 0.1233 - val_accuracy: 0.9675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fac302c1eb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(32, activation='relu', input_dim=20))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(16, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(8, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(4, activation='softmax'))\n",
    "\n",
    "classifier.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier.fit(X_train, y_train, batch_size=32, epochs=300, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
